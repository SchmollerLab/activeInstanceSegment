{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f796e45c-0cb0-4df9-8367-96b60ab4c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "import random as rd\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "import wandb\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.config.config import CfgNode as CN\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "\n",
    "from src.globals import *\n",
    "from src.visualization.show_image import show_image\n",
    "from src.register_datasets import register_datasets, register_by_ids\n",
    "from src.test import do_test\n",
    "from src.train import do_train\n",
    "from src.predict import predict_image_in_acdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdd9bd4a-ad68-4883-81e4-e6bf9bf4a485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_config(config_name):\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "    cfg.NAME = config_name\n",
    "    cfg.AL = CN()\n",
    "    cfg.AL.DATASETS = CN()\n",
    "    cfg.AL.DATASETS.TRAIN_UNLABELED = TRAIN_DATASET_FULL\n",
    "    cfg.AL.MAX_LOOPS = 20\n",
    "    cfg.AL.INIT_SIZE = 20\n",
    "    cfg.AL.INCREMENT_SIZE = 20\n",
    "    cfg.AL.QUERY_STRATEGY = RANDOM\n",
    "    \n",
    "    cfg.DATASETS.TRAIN = (TRAIN_DATASET_FULL,)    \n",
    "    cfg.DATASETS.TEST = (VALIDATION_DATASET_SLIM,)\n",
    "    cfg.DATALOADER.NUM_WORKERS = 2\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "    cfg.SOLVER.IMS_PER_BATCH = 2  # This is the real \"batch size\" commonly known to deep learning people\n",
    "    cfg.SOLVER.BASE_LR = 0.0003  # pick a good LR\n",
    "    cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
    "    cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
    "    cfg.WARMUP_ITERS = 1\n",
    "    cfg.EARLY_STOPPING_ROUNDS = 2\n",
    "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # The \"RoIHead batch size\". 128 is faster, and good enough for this toy dataset (default: 512)\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "    cfg.OUTPUT_DIR = \"./output/\" + cfg.NAME\n",
    "    cfg.TEST.EVAL_PERIOD = 100\n",
    "    \n",
    "    print(cfg.WARMUP_ITERS)\n",
    "    \n",
    "    with open(\"./src/pipeline_configs/\" + cfg.NAME + \".yaml\",\"w\") as file:\n",
    "        file.write(cfg.dump())\n",
    "\n",
    "def get_config(config_name):\n",
    "    \n",
    "    cfg = get_cfg()\n",
    "    cfg.NAME = \" \"\n",
    "    cfg.AL = CN()\n",
    "    cfg.AL.DATASETS = CN()\n",
    "    cfg.AL.DATASETS.TRAIN_UNLABELED = \"\"\n",
    "    cfg.AL.MAX_LOOPS = 0\n",
    "    cfg.AL.INIT_SIZE = 0\n",
    "    cfg.AL.INCREMENT_SIZE = 0\n",
    "    cfg.AL.QUERY_STRATEGY = \"\"\n",
    "    cfg.WARMUP_ITERS = 1\n",
    "    cfg.EARLY_STOPPING_ROUNDS = 2\n",
    "    \n",
    "    file_path = \"src/pipeline_configs/\" + config_name + \".yaml\"\n",
    "    cfg.merge_from_file(file_path)\n",
    "    return cfg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f560119c-4719-4509-ae35-bd7b884f31db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "from src.register_datasets import register_datasets, register_by_ids\n",
    "\n",
    "class ActiveLearingDataset:\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.cfg = cfg\n",
    "        \n",
    "        register_datasets()\n",
    "\n",
    "        # get ids of all images\n",
    "        self.unlabeled_ids = [image[\"image_id\"] for image in DatasetCatalog.get(cfg.AL.DATASETS.TRAIN_UNLABELED)]\n",
    "        self.labeled_ids = []\n",
    "        \n",
    "        self.unlabeled_data_name = \"temp_unlabeled_data_al\"\n",
    "        self.labeled_data_name = \"temp_labeled_data_al\"\n",
    "        \n",
    "        self.init_size = cfg.AL.INIT_SIZE\n",
    "        self.increment_size = cfg.AL.INCREMENT_SIZE\n",
    "        \n",
    "        # set seed\n",
    "        rd.seed(1337)\n",
    "        sample_ids = rd.sample(self.unlabeled_ids, self.init_size)\n",
    "        self.update_labeled_data(sample_ids)\n",
    "        self.get_labeled_dataset()\n",
    "        self.get_unlabled_dataset()\n",
    "        \n",
    "    \n",
    "    def remove_data_from_catalog(self,name):\n",
    "        \n",
    "        if name in DatasetCatalog:\n",
    "            DatasetCatalog.remove(name)\n",
    "            MetadataCatalog.remove(name)\n",
    "        \n",
    "        \n",
    "    def get_labeled_dataset(self):\n",
    "        self.remove_data_from_catalog(self.labeled_data_name)\n",
    "        register_by_ids(self.cfg, self.labeled_data_name, self.labeled_ids)\n",
    "        self.cfg.DATASETS.TRAIN = (self.labeled_data_name,)\n",
    "    \n",
    "    def get_unlabled_dataset(self):\n",
    "        self.remove_data_from_catalog(self.unlabeled_data_name)\n",
    "        register_by_ids(self.cfg, self.unlabeled_data_name,self.unlabeled_ids)\n",
    "        self.cfg.AL.DATASETS.TRAIN_UNLABELED = self.unlabeled_data_name\n",
    "    \n",
    "    def update_labeled_data(self, sample_ids):\n",
    "        print(\"update_labeled_data\")\n",
    "        # check if sample_ids are in unlabeled_ids\n",
    "        if not (set(sample_ids) <= set(self.unlabeled_ids)):\n",
    "            raise Exception(\"Some ids ({}) in sample_ids are not contained in unlabeled data pool: {}\".format(len(list(set(sample_ids) - set(self.unlabeled_ids))),list(set(sample_ids) - set(self.unlabeled_ids))[:5])) \n",
    "\n",
    "        self.labeled_ids += sample_ids\n",
    "        self.unlabeled_ids = list(set(self.unlabeled_ids) - set(sample_ids))\n",
    "        \n",
    "        self.get_labeled_dataset()\n",
    "        self.get_unlabled_dataset()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31d0e31c-3dd9-46b4-b3ef-0e81e1b3f6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "class QueryStrategy(object):\n",
    "    \n",
    "    def __init__(self,cfg):\n",
    "        \n",
    "        self.cfg = cfg\n",
    "        \n",
    "    \n",
    "    def sample(self,model, ids):\n",
    "        pass\n",
    "    \n",
    "class RandomSampler(QueryStrategy):\n",
    "    \n",
    "    def sample(self,model, ids):\n",
    "        num_samples = self.cfg.AL.INCREMENT_SIZE        \n",
    "        samples = rd.sample(ids, num_samples)\n",
    "        return samples\n",
    "\n",
    "class GTknownSampler(QueryStrategy):\n",
    "    \n",
    "    def sample(self, model, ids):\n",
    "        num_samples = self.cfg.AL.INCREMENT_SIZE\n",
    "        \n",
    "        id_pool = rd.sample(ids, min(600,len(ids)))\n",
    "        \n",
    "        register_by_ids(self.cfg,\"GTknownSampler_DS\",id_pool)\n",
    "\n",
    "        \n",
    "        evaluator = COCOEvaluator(\"GTknownSampler_DS\", output_dir=self.cfg.OUTPUT_DIR)\n",
    "        data_loader = build_detection_test_loader(self.cfg, \"GTknownSampler_DS\")\n",
    "        inference_on_dataset(model, data_loader, evaluator)\n",
    "\n",
    "\n",
    "        result_array = []\n",
    "        image_ids = [image[\"image_id\"] for image in DatasetCatalog.get(\"GTknownSampler_DS\")]\n",
    "        for image_id in image_ids:\n",
    "            result = evaluator.evaluate(image_id)\n",
    "            result_array.append(result)\n",
    "\n",
    "        aps = np.array([result['segm']['AP'] for result in result_array])\n",
    "        sample_ids = list(np.argsort(aps)[:num_samples])\n",
    "        print(\"max aps: \", aps[sample_ids[0]])\n",
    "        print(\"min aps: \", aps[list(np.argsort(aps)[:num_samples])[-1]])\n",
    "        \n",
    "        samples = [image_ids[id] for id in sample_ids]\n",
    "\n",
    "        return samples\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b17e6749-87d6-4145-874e-dd06daffa29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ActiveLearningTrainer:\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        # initialize weights and biases\n",
    "        wandb.init(project=\"activeCell-ACDC\", sync_tensorboard=True)\n",
    "        \n",
    "        self.logger = setup_logger(output=\"./log/main.log\")\n",
    "        self.logger.setLevel(10)\n",
    "        \n",
    "        self.al_dataset = ActiveLearingDataset(cfg)   \n",
    "        self.model = build_model(cfg)\n",
    "        self.query_strategy = GTknownSampler(cfg)\n",
    "        \n",
    "        \n",
    "    def __del__(self):\n",
    "        wandb.run.finish()\n",
    "    \n",
    "    def step(self, resume):\n",
    "        \n",
    "        len_ds_train = len(DatasetCatalog.get(self.cfg.DATASETS.TRAIN[0]))\n",
    "        print(\"lenght of train data set: {}\".format(len_ds_train))\n",
    "        self.cfg.SOLVER.MAX_ITER = min(400 + len_ds_train*5, 1000)\n",
    "        self.cfg.SOLVER.STEPS = [math.ceil(self.cfg.SOLVER.MAX_ITER/3),math.ceil(2*self.cfg.SOLVER.MAX_ITER/3)]\n",
    "        \n",
    "        if not resume:\n",
    "            cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "            \n",
    "        do_train(self.cfg, self.model, self.logger,resume=resume)\n",
    "        result = do_test(self.cfg, self.model, self.logger)\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"active_step_bbox_ap\": result['bbox']['AP'],\n",
    "                \"active_step_segm_ap\": result['segm']['AP']\n",
    "            })\n",
    "        \n",
    "\n",
    "        sample_ids = self.query_strategy.sample(self.model, self.al_dataset.unlabeled_ids)\n",
    "        self.al_dataset.update_labeled_data(sample_ids)\n",
    "        \n",
    "    \n",
    "    def run(self):\n",
    "        try:\n",
    "            for i in range(self.cfg.AL.MAX_LOOPS):\n",
    "                self.step(resume=False)    #(i>0))\n",
    "        except Exception as e:\n",
    "            wandb.run.finish()\n",
    "            raise e\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "791b9445-7aa6-43ab-a6e6-ef0ba0ce9510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build_config(\"al_pipeline_config2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cba47561-2b0b-4853-b1e9-81d20f3f7f47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mflorian-bridges\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/florian/GitRepos/activeCell-ACDC/wandb/run-20221111_105213-2pjyrpac</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/florian-bridges/activeCell-ACDC/runs/2pjyrpac\" target=\"_blank\">copper-sun-149</a></strong> to <a href=\"https://wandb.ai/florian-bridges/activeCell-ACDC\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/11 10:52:16 d2.data.datasets.coco]: \u001b[0mLoading ./data/dataInCOCO/train/cell_acdc_coco_ds.json takes 1.95 seconds.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/11 10:52:16 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[11/11 10:52:16 d2.data.datasets.coco]: \u001b[0mLoaded 5784 images in COCO format from ./data/dataInCOCO/train/cell_acdc_coco_ds.json\n",
      "update_labeled_data\n",
      "[Errno 2] No such file or directory: './output/al_pipeline_config2/temp_labeled_data_al_coco_format.json'\n",
      "[Errno 2] No such file or directory: './output/al_pipeline_config2/temp_unlabeled_data_al_coco_format.json'\n",
      "[Errno 2] No such file or directory: './output/al_pipeline_config2/temp_labeled_data_al_coco_format.json'\n",
      "[Errno 2] No such file or directory: './output/al_pipeline_config2/temp_unlabeled_data_al_coco_format.json'\n",
      "\u001b[32m[11/11 10:52:19 d2.data.datasets.coco]: \u001b[0mLoading ./data/dataInCOCO/train/cell_acdc_coco_ds.json takes 1.98 seconds.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/11 10:52:19 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[11/11 10:52:19 d2.data.datasets.coco]: \u001b[0mLoaded 5784 images in COCO format from ./data/dataInCOCO/train/cell_acdc_coco_ds.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florian/GitRepos/activeCell-ACDC/src/train.py:21: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  wandb.config.update(yaml.load(cfg.dump()))\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenght of train data set: 20\n",
      "\u001b[32m[11/11 10:52:22 d2.data.datasets.coco]: \u001b[0mLoading ./data/dataInCOCO/train/cell_acdc_coco_ds.json takes 2.04 seconds.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/11 10:52:22 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[11/11 10:52:22 d2.data.datasets.coco]: \u001b[0mLoaded 5784 images in COCO format from ./data/dataInCOCO/train/cell_acdc_coco_ds.json\n",
      "\u001b[32m[11/11 10:52:22 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 20 images left.\n",
      "\u001b[32m[11/11 10:52:22 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|    cell    | 221          |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[11/11 10:52:22 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[11/11 10:52:22 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[11/11 10:52:22 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>\n",
      "\u001b[32m[11/11 10:52:22 d2.data.common]: \u001b[0mSerializing 20 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[11/11 10:52:22 d2.data.common]: \u001b[0mSerialized dataset takes 0.37 MiB\n",
      "\u001b[32m[11/11 10:52:22 detectron2]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florian/.local/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/11 10:52:30 d2.utils.events]: \u001b[0m iter: 19  total_loss: 2.025  loss_cls: 0.4582  loss_box_reg: 0.8832  loss_mask: 0.5318  loss_rpn_cls: 0.05286  loss_rpn_loc: 0.03516  lr: 0.0003  max_mem: 2668M\n",
      "\u001b[32m[11/11 10:52:36 d2.utils.events]: \u001b[0m eta: 0:02:13  iter: 39  total_loss: 1.27  loss_cls: 0.2256  loss_box_reg: 0.7229  loss_mask: 0.2937  loss_rpn_cls: 0.01092  loss_rpn_loc: 0.03249  lr: 0.0003  max_mem: 2760M\n",
      "\u001b[32m[11/11 10:52:42 d2.utils.events]: \u001b[0m eta: 0:02:12  iter: 59  total_loss: 0.7745  loss_cls: 0.1286  loss_box_reg: 0.3921  loss_mask: 0.18  loss_rpn_cls: 0.00589  loss_rpn_loc: 0.02989  lr: 0.0003  max_mem: 2760M\n",
      "\u001b[32m[11/11 10:52:47 d2.utils.events]: \u001b[0m eta: 0:01:54  iter: 79  total_loss: 0.5291  loss_cls: 0.08736  loss_box_reg: 0.2667  loss_mask: 0.1473  loss_rpn_cls: 0.007222  loss_rpn_loc: 0.03061  lr: 0.0003  max_mem: 2760M\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/11 10:52:53 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[11/11 10:52:53 d2.data.datasets.coco]: \u001b[0mLoaded 241 images in COCO format from ./data/dataInCOCO/test/cell_acdc_coco_ds.json\n",
      "\u001b[32m[11/11 10:52:53 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|    cell    | 851          |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[11/11 10:52:53 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[11/11 10:52:53 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>\n",
      "\u001b[32m[11/11 10:52:53 d2.data.common]: \u001b[0mSerializing 100 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[11/11 10:52:53 d2.data.common]: \u001b[0mSerialized dataset takes 1.44 MiB\n",
      "\u001b[32m[11/11 10:52:53 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
      "\u001b[32m[11/11 10:52:53 d2.evaluation.coco_evaluation]: \u001b[0mTrying to convert 'cell_acdc_validation_slim' to COCO format ...\n",
      "\u001b[32m[11/11 10:52:53 d2.data.datasets.coco]: \u001b[0mConverting annotations of dataset 'cell_acdc_validation_slim' to COCO format ...)\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/11 10:52:53 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[11/11 10:52:53 d2.data.datasets.coco]: \u001b[0mLoaded 241 images in COCO format from ./data/dataInCOCO/test/cell_acdc_coco_ds.json\n",
      "\u001b[32m[11/11 10:52:53 d2.data.datasets.coco]: \u001b[0mConverting dataset dicts into COCO format\n",
      "\u001b[32m[11/11 10:52:53 d2.data.datasets.coco]: \u001b[0mConversion finished, #images: 100, #annotations: 851\n",
      "\u001b[32m[11/11 10:52:53 d2.data.datasets.coco]: \u001b[0mCaching COCO format annotations at './output/al_pipeline_config2/cell_acdc_validation_slim_coco_format.json' ...\n",
      "\u001b[32m[11/11 10:52:53 d2.evaluation.evaluator]: \u001b[0mStart inference on 100 batches\n",
      "\u001b[32m[11/11 10:52:54 d2.evaluation.evaluator]: \u001b[0mInference done 11/100. Dataloading: 0.0006 s/iter. Inference: 0.0630 s/iter. Eval: 0.0009 s/iter. Total: 0.0646 s/iter. ETA=0:00:05\n",
      "\u001b[32m[11/11 10:52:59 d2.evaluation.evaluator]: \u001b[0mInference done 89/100. Dataloading: 0.0007 s/iter. Inference: 0.0632 s/iter. Eval: 0.0007 s/iter. Total: 0.0647 s/iter. ETA=0:00:00\n",
      "\u001b[32m[11/11 10:52:59 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:06.171944 (0.064968 s / iter per device, on 1 devices)\n",
      "\u001b[32m[11/11 10:52:59 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:05 (0.063141 s / iter per device, on 1 devices)\n",
      "\u001b[32m[11/11 10:52:59 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[11/11 10:52:59 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/al_pipeline_config2/coco_instances_results.json\n",
      "\u001b[32m[11/11 10:52:59 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.20s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.787\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.978\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.919\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.788\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.101\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.684\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.821\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.821\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "\u001b[32m[11/11 10:53:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:-----:|:-----:|\n",
      "| 78.743 | 97.779 | 91.939 | 78.760 |  nan  |  nan  |\n",
      "\u001b[32m[11/11 10:53:00 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=0.22s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.789\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.977\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.915\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.789\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.101\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.680\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.817\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.817\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "\u001b[32m[11/11 10:53:00 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:-----:|:-----:|\n",
      "| 78.937 | 97.744 | 91.523 | 78.938 |  nan  |  nan  |\n",
      "\u001b[32m[11/11 10:53:00 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[11/11 10:53:00 detectron2]: \u001b[0mEvaluation results for cell_acdc_validation_slim in csv format:\n",
      "\u001b[32m[11/11 10:53:00 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[11/11 10:53:00 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[11/11 10:53:00 d2.evaluation.testing]: \u001b[0mcopypaste: 78.7427,97.7789,91.9387,78.7602,nan,nan\n",
      "\u001b[32m[11/11 10:53:00 d2.evaluation.testing]: \u001b[0mcopypaste: Task: segm\n",
      "\u001b[32m[11/11 10:53:00 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[11/11 10:53:00 d2.evaluation.testing]: \u001b[0mcopypaste: 78.9372,97.7444,91.5231,78.9379,nan,nan\n",
      "\u001b[32m[11/11 10:53:00 detectron2]: \u001b[0mOrderedDict([('bbox', {'AP': 78.74273764416239, 'AP50': 97.77886264738562, 'AP75': 91.93874877969202, 'APs': 78.76016246397143, 'APm': nan, 'APl': nan}), ('segm', {'AP': 78.9372083211969, 'AP50': 97.7443851664651, 'AP75': 91.52310080282575, 'APs': 78.93785734939603, 'APm': nan, 'APl': nan})])\n",
      "\u001b[32m[11/11 10:53:00 d2.utils.events]: \u001b[0m eta: 0:04:18  iter: 99  total_loss: 0.4965  loss_cls: 0.09568  loss_box_reg: 0.2205  loss_mask: 0.1356  loss_rpn_cls: 0.002963  loss_rpn_loc: 0.02753  lr: 0.0003  max_mem: 2760M\n",
      "\u001b[32m[11/11 10:53:05 d2.utils.events]: \u001b[0m eta: 0:01:44  iter: 119  total_loss: 0.4498  loss_cls: 0.08252  loss_box_reg: 0.1952  loss_mask: 0.1297  loss_rpn_cls: 0.001874  loss_rpn_loc: 0.02648  lr: 0.0003  max_mem: 2760M\n",
      "\u001b[32m[11/11 10:53:11 d2.utils.events]: \u001b[0m eta: 0:01:40  iter: 139  total_loss: 0.4374  loss_cls: 0.09004  loss_box_reg: 0.1846  loss_mask: 0.126  loss_rpn_cls: 0.00102  loss_rpn_loc: 0.02386  lr: 0.0003  max_mem: 2760M\n",
      "\u001b[32m[11/11 10:53:17 d2.utils.events]: \u001b[0m eta: 0:01:36  iter: 159  total_loss: 0.4491  loss_cls: 0.08406  loss_box_reg: 0.1944  loss_mask: 0.1262  loss_rpn_cls: 0.001709  loss_rpn_loc: 0.02276  lr: 0.0003  max_mem: 2760M\n",
      "\u001b[32m[11/11 10:53:22 d2.utils.events]: \u001b[0m eta: 0:01:29  iter: 179  total_loss: 0.403  loss_cls: 0.06599  loss_box_reg: 0.1763  loss_mask: 0.1232  loss_rpn_cls: 0.001518  loss_rpn_loc: 0.02578  lr: 3e-05  max_mem: 2760M\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/11 10:53:28 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[11/11 10:53:28 d2.data.datasets.coco]: \u001b[0mLoaded 241 images in COCO format from ./data/dataInCOCO/test/cell_acdc_coco_ds.json\n",
      "\u001b[32m[11/11 10:53:28 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[11/11 10:53:28 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>\n",
      "\u001b[32m[11/11 10:53:28 d2.data.common]: \u001b[0mSerializing 100 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[11/11 10:53:28 d2.data.common]: \u001b[0mSerialized dataset takes 1.44 MiB\n",
      "\u001b[32m[11/11 10:53:28 d2.evaluation.coco_evaluation]: \u001b[0mFast COCO eval is not built. Falling back to official COCO eval.\n",
      "\u001b[32m[11/11 10:53:28 d2.evaluation.evaluator]: \u001b[0mStart inference on 100 batches\n",
      "\u001b[32m[11/11 10:53:29 d2.evaluation.evaluator]: \u001b[0mInference done 11/100. Dataloading: 0.0006 s/iter. Inference: 0.0625 s/iter. Eval: 0.0009 s/iter. Total: 0.0639 s/iter. ETA=0:00:05\n",
      "\u001b[32m[11/11 10:53:34 d2.evaluation.evaluator]: \u001b[0mInference done 89/100. Dataloading: 0.0008 s/iter. Inference: 0.0632 s/iter. Eval: 0.0007 s/iter. Total: 0.0646 s/iter. ETA=0:00:00\n",
      "\u001b[32m[11/11 10:53:35 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:06.159272 (0.064834 s / iter per device, on 1 devices)\n",
      "\u001b[32m[11/11 10:53:35 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:05 (0.063049 s / iter per device, on 1 devices)\n",
      "\u001b[32m[11/11 10:53:35 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[11/11 10:53:35 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/al_pipeline_config2/coco_instances_results.json\n",
      "\u001b[32m[11/11 10:53:35 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with official COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.19s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.801\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.979\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.919\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.801\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.105\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.694\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.831\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.831\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "\u001b[32m[11/11 10:53:35 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:-----:|:-----:|\n",
      "| 80.124 | 97.922 | 91.887 | 80.124 |  nan  |  nan  |\n",
      "\u001b[32m[11/11 10:53:35 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=0.21s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.978\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.918\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.104\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.686\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.820\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.820\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "\u001b[32m[11/11 10:53:35 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:-----:|:-----:|\n",
      "| 79.774 | 97.840 | 91.823 | 79.774 |  nan  |  nan  |\n",
      "\u001b[32m[11/11 10:53:35 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[11/11 10:53:35 detectron2]: \u001b[0mEvaluation results for cell_acdc_validation_slim in csv format:\n",
      "\u001b[32m[11/11 10:53:35 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[11/11 10:53:35 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[11/11 10:53:35 d2.evaluation.testing]: \u001b[0mcopypaste: 80.1241,97.9225,91.8868,80.1242,nan,nan\n",
      "\u001b[32m[11/11 10:53:35 d2.evaluation.testing]: \u001b[0mcopypaste: Task: segm\n",
      "\u001b[32m[11/11 10:53:35 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[11/11 10:53:35 d2.evaluation.testing]: \u001b[0mcopypaste: 79.7742,97.8400,91.8230,79.7742,nan,nan\n",
      "\u001b[32m[11/11 10:53:35 detectron2]: \u001b[0mOrderedDict([('bbox', {'AP': 80.12406728812206, 'AP50': 97.92248227471602, 'AP75': 91.88676916118179, 'APs': 80.1241636914347, 'APm': nan, 'APl': nan}), ('segm', {'AP': 79.77417586894816, 'AP50': 97.83998875492621, 'AP75': 91.82303504539495, 'APs': 79.77417586894816, 'APm': nan, 'APl': nan})])\n",
      "\u001b[32m[11/11 10:53:35 d2.utils.events]: \u001b[0m eta: 0:03:10  iter: 199  total_loss: 0.4143  loss_cls: 0.0776  loss_box_reg: 0.1738  loss_mask: 0.1262  loss_rpn_cls: 0.001167  loss_rpn_loc: 0.02059  lr: 3e-05  max_mem: 2760M\n",
      "\u001b[32m[11/11 10:53:41 d2.utils.events]: \u001b[0m eta: 0:01:20  iter: 219  total_loss: 0.3673  loss_cls: 0.07126  loss_box_reg: 0.1724  loss_mask: 0.1197  loss_rpn_cls: 0.00192  loss_rpn_loc: 0.02084  lr: 3e-05  max_mem: 2760M\n",
      "\u001b[32m[11/11 10:53:47 d2.utils.events]: \u001b[0m eta: 0:01:15  iter: 239  total_loss: 0.392  loss_cls: 0.07173  loss_box_reg: 0.1707  loss_mask: 0.1207  loss_rpn_cls: 0.001876  loss_rpn_loc: 0.01918  lr: 3e-05  max_mem: 2760M\n",
      "\u001b[32m[11/11 10:53:52 d2.utils.events]: \u001b[0m eta: 0:01:06  iter: 259  total_loss: 0.3827  loss_cls: 0.07173  loss_box_reg: 0.1648  loss_mask: 0.122  loss_rpn_cls: 0.001188  loss_rpn_loc: 0.02104  lr: 3e-05  max_mem: 2760M\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m cfg \u001b[38;5;241m=\u001b[39m get_config(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mal_pipeline_config2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m al_trainer \u001b[38;5;241m=\u001b[39m ActiveLearningTrainer(cfg)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mal_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mActiveLearningTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mAL\u001b[38;5;241m.\u001b[39mMAX_LOOPS):\n\u001b[0;32m---> 46\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m#(i>0))\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     48\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mActiveLearningTrainer.step\u001b[0;34m(self, resume)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resume:\n\u001b[1;32m     28\u001b[0m     cfg\u001b[38;5;241m.\u001b[39mMODEL\u001b[38;5;241m.\u001b[39mWEIGHTS \u001b[38;5;241m=\u001b[39m model_zoo\u001b[38;5;241m.\u001b[39mget_checkpoint_url(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mdo_train\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43mresume\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m result \u001b[38;5;241m=\u001b[39m do_test(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger)\n\u001b[1;32m     32\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m     33\u001b[0m     {\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactive_step_bbox_ap\u001b[39m\u001b[38;5;124m\"\u001b[39m: result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAP\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactive_step_segm_ap\u001b[39m\u001b[38;5;124m\"\u001b[39m: result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msegm\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAP\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     36\u001b[0m     })\n",
      "File \u001b[0;32m~/GitRepos/activeCell-ACDC/src/train.py:59\u001b[0m, in \u001b[0;36mdo_train\u001b[0;34m(cfg, model, logger, resume)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(data_loader, \u001b[38;5;28mrange\u001b[39m(start_iter, max_iter)):\n\u001b[1;32m     57\u001b[0m     storage\u001b[38;5;241m.\u001b[39miter \u001b[38;5;241m=\u001b[39m iteration\n\u001b[0;32m---> 59\u001b[0m     loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misfinite(losses)\u001b[38;5;241m.\u001b[39mall(), loss_dict\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py:152\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(batched_inputs)\n\u001b[0;32m--> 152\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batched_inputs[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m    154\u001b[0m     gt_instances \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batched_inputs]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py:227\u001b[0m, in \u001b[0;36mGeneralizedRCNN.preprocess_image\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, batched_inputs: List[Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]]):\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m    Normalize, pad and batch the input images.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_move_to_current_device(x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batched_inputs]\n\u001b[1;32m    228\u001b[0m     images \u001b[38;5;241m=\u001b[39m [(x \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpixel_mean) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpixel_std \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    229\u001b[0m     images \u001b[38;5;241m=\u001b[39m ImageList\u001b[38;5;241m.\u001b[39mfrom_tensors(\n\u001b[1;32m    230\u001b[0m         images,\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39msize_divisibility,\n\u001b[1;32m    232\u001b[0m         padding_constraints\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mpadding_constraints,\n\u001b[1;32m    233\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py:227\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, batched_inputs: List[Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]]):\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m    Normalize, pad and batch the input images.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_to_current_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batched_inputs]\n\u001b[1;32m    228\u001b[0m     images \u001b[38;5;241m=\u001b[39m [(x \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpixel_mean) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpixel_std \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    229\u001b[0m     images \u001b[38;5;241m=\u001b[39m ImageList\u001b[38;5;241m.\u001b[39mfrom_tensors(\n\u001b[1;32m    230\u001b[0m         images,\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39msize_divisibility,\n\u001b[1;32m    232\u001b[0m         padding_constraints\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mpadding_constraints,\n\u001b[1;32m    233\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py:89\u001b[0m, in \u001b[0;36mGeneralizedRCNN._move_to_current_device\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_to_current_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmove_device_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpixel_mean\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/jit/_trace.py:1136\u001b[0m, in \u001b[0;36m_script_if_tracing.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing():\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# Not tracing, don't do anything\u001b[39;00m\n\u001b[0;32m-> 1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1138\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m script(wrapper\u001b[38;5;241m.\u001b[39m__original_fn)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/detectron2/layers/wrappers.py:148\u001b[0m, in \u001b[0;36mmove_device_like\u001b[0;34m(src, dst)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mscript_if_tracing\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmove_device_like\u001b[39m(src: torch\u001b[38;5;241m.\u001b[39mTensor, dst: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    Tracing friendly way to cast tensor to another tensor's device. Device will be treated\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    as constant during tracing, scripting the casting process as whole can workaround this issue.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "cfg = get_config(\"al_pipeline_config2\")\n",
    "al_trainer = ActiveLearningTrainer(cfg)\n",
    "al_trainer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d364eb-d55a-46a8-9d41-0e205373bb1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
