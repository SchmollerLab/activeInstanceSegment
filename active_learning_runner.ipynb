{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f796e45c-0cb0-4df9-8367-96b60ab4c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "import random as rd\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "import wandb\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.config.config import CfgNode as CN\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "\n",
    "from src.globals import *\n",
    "from src.visualization.show_image import show_image\n",
    "from src.register_datasets import register_datasets, register_by_ids\n",
    "from src.test import do_test\n",
    "from src.train import do_train\n",
    "from src.predict import predict_image_in_acdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdd9bd4a-ad68-4883-81e4-e6bf9bf4a485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_config(config_name):\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "    cfg.NAME = config_name\n",
    "    cfg.AL = CN()\n",
    "    cfg.AL.DATASETS = CN()\n",
    "    cfg.AL.DATASETS.TRAIN_UNLABELED = TRAIN_DATASET_FULL\n",
    "    cfg.AL.MAX_LOOPS = 20\n",
    "    cfg.AL.INIT_SIZE = 20\n",
    "    cfg.AL.INCREMENT_SIZE = 20\n",
    "    cfg.AL.QUERY_STRATEGY = RANDOM\n",
    "    \n",
    "    cfg.DATASETS.TRAIN = (TRAIN_DATASET_FULL,)    \n",
    "    cfg.DATASETS.TEST = (VALIDATION_DATASET_SLIM,)\n",
    "    cfg.DATALOADER.NUM_WORKERS = 2\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "    cfg.SOLVER.IMS_PER_BATCH = 2  # This is the real \"batch size\" commonly known to deep learning people\n",
    "    cfg.SOLVER.BASE_LR = 0.0003  # pick a good LR\n",
    "    cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
    "    cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
    "    cfg.WARMUP_ITERS = 1\n",
    "    cfg.EARLY_STOPPING_ROUNDS = 2\n",
    "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # The \"RoIHead batch size\". 128 is faster, and good enough for this toy dataset (default: 512)\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "    cfg.OUTPUT_DIR = \"./output/\" + cfg.NAME\n",
    "    cfg.TEST.EVAL_PERIOD = 100\n",
    "    \n",
    "    print(cfg.WARMUP_ITERS)\n",
    "    \n",
    "    with open(\"./src/pipeline_configs/\" + cfg.NAME + \".yaml\",\"w\") as file:\n",
    "        file.write(cfg.dump())\n",
    "\n",
    "def get_config(config_name):\n",
    "    \n",
    "    cfg = get_cfg()\n",
    "    cfg.NAME = \" \"\n",
    "    cfg.AL = CN()\n",
    "    cfg.AL.DATASETS = CN()\n",
    "    cfg.AL.DATASETS.TRAIN_UNLABELED = \"\"\n",
    "    cfg.AL.MAX_LOOPS = 0\n",
    "    cfg.AL.INIT_SIZE = 0\n",
    "    cfg.AL.INCREMENT_SIZE = 0\n",
    "    cfg.AL.QUERY_STRATEGY = \"\"\n",
    "    cfg.WARMUP_ITERS = 1\n",
    "    cfg.EARLY_STOPPING_ROUNDS = 2\n",
    "    \n",
    "    file_path = \"src/pipeline_configs/\" + config_name + \".yaml\"\n",
    "    cfg.merge_from_file(file_path)\n",
    "    return cfg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f560119c-4719-4509-ae35-bd7b884f31db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "from src.register_datasets import register_datasets, register_by_ids\n",
    "\n",
    "class ActiveLearingDataset:\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.cfg = cfg\n",
    "        \n",
    "        register_datasets()\n",
    "\n",
    "        # get ids of all images\n",
    "        self.unlabeled_ids = [image[\"image_id\"] for image in DatasetCatalog.get(cfg.AL.DATASETS.TRAIN_UNLABELED)]\n",
    "        self.labeled_ids = []\n",
    "        \n",
    "        self.unlabeled_data_name = \"temp_unlabeled_data_al\"\n",
    "        self.labeled_data_name = \"temp_labeled_data_al\"\n",
    "        \n",
    "        self.init_size = cfg.AL.INIT_SIZE\n",
    "        self.increment_size = cfg.AL.INCREMENT_SIZE\n",
    "        \n",
    "        # set seed\n",
    "        rd.seed(1337)\n",
    "        sample_ids = rd.sample(self.unlabeled_ids, self.init_size)\n",
    "        self.update_labeled_data(sample_ids)\n",
    "        self.get_labeled_dataset()\n",
    "        self.get_unlabled_dataset()\n",
    "        \n",
    "    \n",
    "    def remove_data_from_catalog(self,name):\n",
    "        \n",
    "        if name in DatasetCatalog:\n",
    "            DatasetCatalog.remove(name)\n",
    "            MetadataCatalog.remove(name)\n",
    "        \n",
    "        \n",
    "    def get_labeled_dataset(self):\n",
    "        self.remove_data_from_catalog(self.labeled_data_name)\n",
    "        register_by_ids(self.cfg, self.labeled_data_name, self.labeled_ids)\n",
    "        self.cfg.DATASETS.TRAIN = (self.labeled_data_name,)\n",
    "    \n",
    "    def get_unlabled_dataset(self):\n",
    "        self.remove_data_from_catalog(self.unlabeled_data_name)\n",
    "        register_by_ids(self.cfg, self.unlabeled_data_name,self.unlabeled_ids)\n",
    "        self.cfg.AL.DATASETS.TRAIN_UNLABELED = self.unlabeled_data_name\n",
    "    \n",
    "    def update_labeled_data(self, sample_ids):\n",
    "        print(\"update_labeled_data\")\n",
    "        # check if sample_ids are in unlabeled_ids\n",
    "        if not (set(sample_ids) <= set(self.unlabeled_ids)):\n",
    "            raise Exception(\"Some ids ({}) in sample_ids are not contained in unlabeled data pool: {}\".format(len(list(set(sample_ids) - set(self.unlabeled_ids))),list(set(sample_ids) - set(self.unlabeled_ids))[:5])) \n",
    "\n",
    "        self.labeled_ids += sample_ids\n",
    "        self.unlabeled_ids = list(set(self.unlabeled_ids) - set(sample_ids))\n",
    "        \n",
    "        self.get_labeled_dataset()\n",
    "        self.get_unlabled_dataset()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31d0e31c-3dd9-46b4-b3ef-0e81e1b3f6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "class QueryStrategy(object):\n",
    "    \n",
    "    def __init__(self,cfg):\n",
    "        \n",
    "        self.cfg = cfg\n",
    "        \n",
    "    \n",
    "    def sample(self,model, ids):\n",
    "        pass\n",
    "    \n",
    "class RandomSampler(QueryStrategy):\n",
    "    \n",
    "    def sample(self,model, ids):\n",
    "        num_samples = self.cfg.AL.INCREMENT_SIZE        \n",
    "        samples = rd.sample(ids, num_samples)\n",
    "        return samples\n",
    "\n",
    "class GTknownSampler(QueryStrategy):\n",
    "    \n",
    "    def sample(self, model, ids):\n",
    "        num_samples = self.cfg.AL.INCREMENT_SIZE\n",
    "        \n",
    "        id_pool = rd.sample(ids, min(600,len(ids)))\n",
    "        \n",
    "        register_by_ids(self.cfg,\"GTknownSampler_DS\",id_pool)\n",
    "\n",
    "        \n",
    "        evaluator = COCOEvaluator(\"GTknownSampler_DS\", output_dir=self.cfg.OUTPUT_DIR)\n",
    "        data_loader = build_detection_test_loader(self.cfg, \"GTknownSampler_DS\")\n",
    "        inference_on_dataset(model, data_loader, evaluator)\n",
    "\n",
    "\n",
    "        result_array = []\n",
    "        image_ids = [image[\"image_id\"] for image in DatasetCatalog.get(\"GTknownSampler_DS\")]\n",
    "        for image_id in image_ids:\n",
    "            result = evaluator.evaluate(image_id)\n",
    "            result_array.append(result)\n",
    "\n",
    "        aps = np.array([result['segm']['AP'] for result in result_array])\n",
    "        sample_ids = list(np.argsort(aps)[:num_samples])\n",
    "        print(\"max aps: \", aps[sample_ids[0]])\n",
    "        print(\"min aps: \", aps[list(np.argsort(aps)[:num_samples])[-1]])\n",
    "        \n",
    "        samples = [image_ids[id] for id in sample_ids]\n",
    "\n",
    "        return samples\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b17e6749-87d6-4145-874e-dd06daffa29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ActiveLearningTrainer:\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        # initialize weights and biases\n",
    "        wandb.init(project=\"activeCell-ACDC\", sync_tensorboard=True)\n",
    "        \n",
    "        self.logger = setup_logger(output=\"./log/main.log\")\n",
    "        self.logger.setLevel(10)\n",
    "        \n",
    "        self.al_dataset = ActiveLearingDataset(cfg)   \n",
    "        self.model = build_model(cfg)\n",
    "        self.query_strategy = GTknownSampler(cfg)\n",
    "        \n",
    "        \n",
    "    def __del__(self):\n",
    "        wandb.run.finish()\n",
    "    \n",
    "    def step(self, resume):\n",
    "        \n",
    "        len_ds_train = len(DatasetCatalog.get(self.cfg.DATASETS.TRAIN[0]))\n",
    "        print(\"lenght of train data set: {}\".format(len_ds_train))\n",
    "        self.cfg.SOLVER.MAX_ITER = min(400 + len_ds_train*5, 1000)\n",
    "        self.cfg.SOLVER.STEPS = [math.ceil(self.cfg.SOLVER.MAX_ITER/3),math.ceil(2*self.cfg.SOLVER.MAX_ITER/3)]\n",
    "        \n",
    "        if not resume:\n",
    "            cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "            \n",
    "        do_train(self.cfg, self.model, self.logger,resume=resume)\n",
    "        result = do_test(self.cfg, self.model, self.logger)\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"active_step_bbox_ap\": result['bbox']['AP'],\n",
    "                \"active_step_segm_ap\": result['segm']['AP']\n",
    "            })\n",
    "        \n",
    "\n",
    "        sample_ids = self.query_strategy.sample(self.model, self.al_dataset.unlabeled_ids)\n",
    "        self.al_dataset.update_labeled_data(sample_ids)\n",
    "        \n",
    "    \n",
    "    def run(self):\n",
    "        try:\n",
    "            for i in range(self.cfg.AL.MAX_LOOPS):\n",
    "                self.step(resume=False)    #(i>0))\n",
    "        except Exception as e:\n",
    "            wandb.run.finish()\n",
    "            raise e\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "791b9445-7aa6-43ab-a6e6-ef0ba0ce9510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build_config(\"al_pipeline_config2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba47561-2b0b-4853-b1e9-81d20f3f7f47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mflorian-bridges\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/GitRepos/activeCell-ACDC/wandb/run-20221110_172158-20vb93qk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/florian-bridges/activeCell-ACDC/runs/20vb93qk\" target=\"_blank\">likely-monkey-147</a></strong> to <a href=\"https://wandb.ai/florian-bridges/activeCell-ACDC\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/10 17:22:04 d2.data.datasets.coco]: \u001b[0mLoading ./data/dataInCOCO/train/cell_acdc_coco_ds.json takes 3.39 seconds.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/10 17:22:04 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[11/10 17:22:04 d2.data.datasets.coco]: \u001b[0mLoaded 5784 images in COCO format from ./data/dataInCOCO/train/cell_acdc_coco_ds.json\n",
      "update_labeled_data\n",
      "[Errno 2] No such file or directory: './output/al_pipeline_config2/temp_labeled_data_al_coco_format.json'\n",
      "[Errno 2] No such file or directory: './output/al_pipeline_config2/temp_unlabeled_data_al_coco_format.json'\n",
      "[Errno 2] No such file or directory: './output/al_pipeline_config2/temp_labeled_data_al_coco_format.json'\n",
      "[Errno 2] No such file or directory: './output/al_pipeline_config2/temp_unlabeled_data_al_coco_format.json'\n",
      "\u001b[32m[11/10 17:22:12 d2.data.datasets.coco]: \u001b[0mLoading ./data/dataInCOCO/train/cell_acdc_coco_ds.json takes 3.41 seconds.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/10 17:22:12 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[11/10 17:22:12 d2.data.datasets.coco]: \u001b[0mLoaded 5784 images in COCO format from ./data/dataInCOCO/train/cell_acdc_coco_ds.json\n",
      "lenght of train data set: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/GitRepos/activeCell-ACDC/src/train.py:21: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  wandb.config.update(yaml.load(cfg.dump()))\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/10 17:22:16 d2.data.datasets.coco]: \u001b[0mLoading ./data/dataInCOCO/train/cell_acdc_coco_ds.json takes 2.64 seconds.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/10 17:22:16 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[11/10 17:22:17 d2.data.datasets.coco]: \u001b[0mLoaded 5784 images in COCO format from ./data/dataInCOCO/train/cell_acdc_coco_ds.json\n",
      "\u001b[32m[11/10 17:22:17 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 20 images left.\n",
      "\u001b[32m[11/10 17:22:17 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|    cell    | 242          |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[11/10 17:22:17 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[11/10 17:22:17 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[11/10 17:22:17 d2.data.common]: \u001b[0mSerializing 20 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[11/10 17:22:17 d2.data.common]: \u001b[0mSerialized dataset takes 0.40 MiB\n",
      "\u001b[32m[11/10 17:22:17 detectron2]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/10 17:22:28 d2.utils.events]: \u001b[0m iter: 19  total_loss: 1.959  loss_cls: 0.4534  loss_box_reg: 0.8879  loss_mask: 0.5438  loss_rpn_cls: 0.05388  loss_rpn_loc: 0.03706  lr: 0.0003  max_mem: 2759M\n",
      "\u001b[32m[11/10 17:22:37 d2.utils.events]: \u001b[0m eta: 0:03:20  iter: 39  total_loss: 1.317  loss_cls: 0.2297  loss_box_reg: 0.7228  loss_mask: 0.284  loss_rpn_cls: 0.009751  loss_rpn_loc: 0.03441  lr: 0.0003  max_mem: 2759M\n",
      "\u001b[32m[11/10 17:22:55 d2.utils.events]: \u001b[0m eta: 0:06:37  iter: 59  total_loss: 0.7468  loss_cls: 0.1249  loss_box_reg: 0.3935  loss_mask: 0.1698  loss_rpn_cls: 0.005578  loss_rpn_loc: 0.02947  lr: 0.0003  max_mem: 2759M\n",
      "\u001b[32m[11/10 17:23:35 d2.utils.events]: \u001b[0m eta: 0:14:01  iter: 79  total_loss: 0.5194  loss_cls: 0.0938  loss_box_reg: 0.2471  loss_mask: 0.1427  loss_rpn_cls: 0.004753  loss_rpn_loc: 0.02784  lr: 0.0003  max_mem: 2759M\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/10 17:24:17 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[11/10 17:24:17 d2.data.datasets.coco]: \u001b[0mLoaded 241 images in COCO format from ./data/dataInCOCO/test/cell_acdc_coco_ds.json\n",
      "\u001b[32m[11/10 17:24:17 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|    cell    | 1454         |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[11/10 17:24:17 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[11/10 17:24:17 d2.data.common]: \u001b[0mSerializing 100 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[11/10 17:24:17 d2.data.common]: \u001b[0mSerialized dataset takes 2.50 MiB\n",
      "\u001b[32m[11/10 17:24:17 d2.evaluation.coco_evaluation]: \u001b[0mTrying to convert 'cell_acdc_validation_slim' to COCO format ...\n",
      "\u001b[32m[11/10 17:24:17 d2.data.datasets.coco]: \u001b[0mConverting annotations of dataset 'cell_acdc_validation_slim' to COCO format ...)\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/10 17:24:18 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[11/10 17:24:18 d2.data.datasets.coco]: \u001b[0mLoaded 241 images in COCO format from ./data/dataInCOCO/test/cell_acdc_coco_ds.json\n",
      "\u001b[32m[11/10 17:24:18 d2.data.datasets.coco]: \u001b[0mConverting dataset dicts into COCO format\n",
      "\u001b[32m[11/10 17:24:18 d2.data.datasets.coco]: \u001b[0mConversion finished, #images: 100, #annotations: 1454\n",
      "\u001b[32m[11/10 17:24:18 d2.data.datasets.coco]: \u001b[0mCaching COCO format annotations at './output/al_pipeline_config2/cell_acdc_validation_slim_coco_format.json' ...\n",
      "\u001b[32m[11/10 17:24:18 d2.evaluation.evaluator]: \u001b[0mStart inference on 100 batches\n",
      "\u001b[32m[11/10 17:24:23 d2.evaluation.evaluator]: \u001b[0mInference done 11/100. Dataloading: 0.0015 s/iter. Inference: 0.4419 s/iter. Eval: 0.0014 s/iter. Total: 0.4448 s/iter. ETA=0:00:39\n",
      "\u001b[32m[11/10 17:24:29 d2.evaluation.evaluator]: \u001b[0mInference done 23/100. Dataloading: 0.0018 s/iter. Inference: 0.4453 s/iter. Eval: 0.0015 s/iter. Total: 0.4489 s/iter. ETA=0:00:34\n",
      "\u001b[32m[11/10 17:24:34 d2.evaluation.evaluator]: \u001b[0mInference done 34/100. Dataloading: 0.0019 s/iter. Inference: 0.4480 s/iter. Eval: 0.0017 s/iter. Total: 0.4519 s/iter. ETA=0:00:29\n",
      "\u001b[32m[11/10 17:24:39 d2.evaluation.evaluator]: \u001b[0mInference done 45/100. Dataloading: 0.0020 s/iter. Inference: 0.4493 s/iter. Eval: 0.0018 s/iter. Total: 0.4533 s/iter. ETA=0:00:24\n",
      "\u001b[32m[11/10 17:24:44 d2.evaluation.evaluator]: \u001b[0mInference done 57/100. Dataloading: 0.0021 s/iter. Inference: 0.4475 s/iter. Eval: 0.0017 s/iter. Total: 0.4515 s/iter. ETA=0:00:19\n",
      "\u001b[32m[11/10 17:24:49 d2.evaluation.evaluator]: \u001b[0mInference done 69/100. Dataloading: 0.0022 s/iter. Inference: 0.4471 s/iter. Eval: 0.0016 s/iter. Total: 0.4512 s/iter. ETA=0:00:13\n",
      "\u001b[32m[11/10 17:24:55 d2.evaluation.evaluator]: \u001b[0mInference done 81/100. Dataloading: 0.0021 s/iter. Inference: 0.4475 s/iter. Eval: 0.0017 s/iter. Total: 0.4516 s/iter. ETA=0:00:08\n",
      "\u001b[32m[11/10 17:25:00 d2.evaluation.evaluator]: \u001b[0mInference done 93/100. Dataloading: 0.0021 s/iter. Inference: 0.4473 s/iter. Eval: 0.0017 s/iter. Total: 0.4513 s/iter. ETA=0:00:03\n",
      "\u001b[32m[11/10 17:25:03 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:42.912282 (0.451708 s / iter per device, on 1 devices)\n",
      "\u001b[32m[11/10 17:25:03 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:42 (0.446714 s / iter per device, on 1 devices)\n",
      "\u001b[32m[11/10 17:25:03 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[11/10 17:25:03 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/al_pipeline_config2/coco_instances_results.json\n",
      "\u001b[32m[11/10 17:25:03 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[11/10 17:25:03 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[11/10 17:25:04 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.19 seconds.\n",
      "\u001b[32m[11/10 17:25:04 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[11/10 17:25:04 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.960\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.910\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.808\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.064\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.515\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.831\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.831\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "\u001b[32m[11/10 17:25:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:-----:|:-----:|\n",
      "| 80.751 | 95.987 | 90.970 | 80.786 |  nan  |  nan  |\n",
      "\u001b[32m[11/10 17:25:04 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[11/10 17:25:04 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *segm*\n",
      "\u001b[32m[11/10 17:25:04 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "\u001b[32m[11/10 17:25:04 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[11/10 17:25:04 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.818\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.960\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.899\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.818\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.064\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.523\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.839\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.839\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "\u001b[32m[11/10 17:25:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:-----:|:-----:|\n",
      "| 81.802 | 95.992 | 89.925 | 81.803 |  nan  |  nan  |\n",
      "\u001b[32m[11/10 17:25:04 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[11/10 17:25:04 detectron2]: \u001b[0mEvaluation results for cell_acdc_validation_slim in csv format:\n",
      "\u001b[32m[11/10 17:25:04 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[11/10 17:25:04 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[11/10 17:25:04 d2.evaluation.testing]: \u001b[0mcopypaste: 80.7511,95.9875,90.9697,80.7860,nan,nan\n",
      "\u001b[32m[11/10 17:25:04 d2.evaluation.testing]: \u001b[0mcopypaste: Task: segm\n",
      "\u001b[32m[11/10 17:25:04 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[11/10 17:25:04 d2.evaluation.testing]: \u001b[0mcopypaste: 81.8023,95.9919,89.9254,81.8029,nan,nan\n",
      "\u001b[32m[11/10 17:25:04 detectron2]: \u001b[0mOrderedDict([('bbox', {'AP': 80.75107727141496, 'AP50': 95.98747538095921, 'AP75': 90.96965985367173, 'APs': 80.78603134723829, 'APm': nan, 'APl': nan}), ('segm', {'AP': 81.80229234935902, 'AP50': 95.99186513613166, 'AP75': 89.92543847551336, 'APs': 81.80291744578776, 'APm': nan, 'APl': nan})])\n",
      "\u001b[32m[11/10 17:25:04 d2.utils.events]: \u001b[0m eta: 0:29:29  iter: 99  total_loss: 0.5075  loss_cls: 0.1073  loss_box_reg: 0.2229  loss_mask: 0.13  loss_rpn_cls: 0.003909  loss_rpn_loc: 0.02423  lr: 0.0003  max_mem: 2759M\n",
      "\u001b[32m[11/10 17:25:45 d2.utils.events]: \u001b[0m eta: 0:12:53  iter: 119  total_loss: 0.4466  loss_cls: 0.08042  loss_box_reg: 0.1965  loss_mask: 0.1327  loss_rpn_cls: 0.00363  loss_rpn_loc: 0.02293  lr: 0.0003  max_mem: 2759M\n",
      "\u001b[32m[11/10 17:26:27 d2.utils.events]: \u001b[0m eta: 0:12:52  iter: 139  total_loss: 0.4535  loss_cls: 0.08792  loss_box_reg: 0.1844  loss_mask: 0.1295  loss_rpn_cls: 0.003216  loss_rpn_loc: 0.01787  lr: 0.0003  max_mem: 2759M\n",
      "\u001b[32m[11/10 17:27:10 d2.utils.events]: \u001b[0m eta: 0:12:00  iter: 159  total_loss: 0.4285  loss_cls: 0.0842  loss_box_reg: 0.1951  loss_mask: 0.1252  loss_rpn_cls: 0.002351  loss_rpn_loc: 0.01868  lr: 0.0003  max_mem: 2759M\n",
      "\u001b[32m[11/10 17:28:02 d2.utils.events]: \u001b[0m eta: 0:13:52  iter: 179  total_loss: 0.4028  loss_cls: 0.076  loss_box_reg: 0.1795  loss_mask: 0.1244  loss_rpn_cls: 0.001381  loss_rpn_loc: 0.01846  lr: 3e-05  max_mem: 2759M\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/10 17:29:22 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[11/10 17:29:22 d2.data.datasets.coco]: \u001b[0mLoaded 241 images in COCO format from ./data/dataInCOCO/test/cell_acdc_coco_ds.json\n",
      "\u001b[32m[11/10 17:29:22 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[11/10 17:29:22 d2.data.common]: \u001b[0mSerializing 100 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[11/10 17:29:22 d2.data.common]: \u001b[0mSerialized dataset takes 2.50 MiB\n",
      "\u001b[32m[11/10 17:29:22 d2.evaluation.evaluator]: \u001b[0mStart inference on 100 batches\n",
      "\u001b[32m[11/10 17:29:33 d2.evaluation.evaluator]: \u001b[0mInference done 11/100. Dataloading: 0.0018 s/iter. Inference: 0.8624 s/iter. Eval: 0.0013 s/iter. Total: 0.8655 s/iter. ETA=0:01:17\n",
      "\u001b[32m[11/10 17:29:39 d2.evaluation.evaluator]: \u001b[0mInference done 17/100. Dataloading: 0.0032 s/iter. Inference: 0.8710 s/iter. Eval: 0.0016 s/iter. Total: 0.8766 s/iter. ETA=0:01:12\n",
      "\u001b[32m[11/10 17:29:44 d2.evaluation.evaluator]: \u001b[0mInference done 23/100. Dataloading: 0.0030 s/iter. Inference: 0.8692 s/iter. Eval: 0.0016 s/iter. Total: 0.8744 s/iter. ETA=0:01:07\n",
      "\u001b[32m[11/10 17:29:49 d2.evaluation.evaluator]: \u001b[0mInference done 29/100. Dataloading: 0.0028 s/iter. Inference: 0.8741 s/iter. Eval: 0.0017 s/iter. Total: 0.8792 s/iter. ETA=0:01:02\n",
      "\u001b[32m[11/10 17:29:54 d2.evaluation.evaluator]: \u001b[0mInference done 35/100. Dataloading: 0.0029 s/iter. Inference: 0.8751 s/iter. Eval: 0.0017 s/iter. Total: 0.8803 s/iter. ETA=0:00:57\n",
      "\u001b[32m[11/10 17:30:00 d2.evaluation.evaluator]: \u001b[0mInference done 41/100. Dataloading: 0.0031 s/iter. Inference: 0.8807 s/iter. Eval: 0.0019 s/iter. Total: 0.8863 s/iter. ETA=0:00:52\n",
      "\u001b[32m[11/10 17:30:05 d2.evaluation.evaluator]: \u001b[0mInference done 47/100. Dataloading: 0.0030 s/iter. Inference: 0.8783 s/iter. Eval: 0.0018 s/iter. Total: 0.8839 s/iter. ETA=0:00:46\n",
      "\u001b[32m[11/10 17:30:10 d2.evaluation.evaluator]: \u001b[0mInference done 53/100. Dataloading: 0.0028 s/iter. Inference: 0.8765 s/iter. Eval: 0.0017 s/iter. Total: 0.8818 s/iter. ETA=0:00:41\n",
      "\u001b[32m[11/10 17:30:16 d2.evaluation.evaluator]: \u001b[0mInference done 59/100. Dataloading: 0.0028 s/iter. Inference: 0.8768 s/iter. Eval: 0.0017 s/iter. Total: 0.8820 s/iter. ETA=0:00:36\n",
      "\u001b[32m[11/10 17:30:21 d2.evaluation.evaluator]: \u001b[0mInference done 65/100. Dataloading: 0.0027 s/iter. Inference: 0.8748 s/iter. Eval: 0.0017 s/iter. Total: 0.8799 s/iter. ETA=0:00:30\n",
      "\u001b[32m[11/10 17:30:26 d2.evaluation.evaluator]: \u001b[0mInference done 71/100. Dataloading: 0.0026 s/iter. Inference: 0.8756 s/iter. Eval: 0.0017 s/iter. Total: 0.8806 s/iter. ETA=0:00:25\n",
      "\u001b[32m[11/10 17:30:31 d2.evaluation.evaluator]: \u001b[0mInference done 77/100. Dataloading: 0.0026 s/iter. Inference: 0.8749 s/iter. Eval: 0.0017 s/iter. Total: 0.8798 s/iter. ETA=0:00:20\n",
      "\u001b[32m[11/10 17:30:37 d2.evaluation.evaluator]: \u001b[0mInference done 83/100. Dataloading: 0.0025 s/iter. Inference: 0.8753 s/iter. Eval: 0.0017 s/iter. Total: 0.8802 s/iter. ETA=0:00:14\n",
      "\u001b[32m[11/10 17:30:42 d2.evaluation.evaluator]: \u001b[0mInference done 89/100. Dataloading: 0.0025 s/iter. Inference: 0.8754 s/iter. Eval: 0.0017 s/iter. Total: 0.8801 s/iter. ETA=0:00:09\n",
      "\u001b[32m[11/10 17:30:47 d2.evaluation.evaluator]: \u001b[0mInference done 95/100. Dataloading: 0.0025 s/iter. Inference: 0.8746 s/iter. Eval: 0.0017 s/iter. Total: 0.8793 s/iter. ETA=0:00:04\n",
      "\u001b[32m[11/10 17:30:52 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:01:23.530766 (0.879271 s / iter per device, on 1 devices)\n",
      "\u001b[32m[11/10 17:30:52 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:23 (0.873731 s / iter per device, on 1 devices)\n",
      "\u001b[32m[11/10 17:30:52 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[11/10 17:30:52 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/al_pipeline_config2/coco_instances_results.json\n",
      "\u001b[32m[11/10 17:30:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[11/10 17:30:52 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[11/10 17:30:52 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.03 seconds.\n",
      "\u001b[32m[11/10 17:30:52 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[11/10 17:30:52 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.824\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.960\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.920\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.824\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.064\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.524\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.850\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.850\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "\u001b[32m[11/10 17:30:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:-----:|:-----:|\n",
      "| 82.364 | 96.026 | 91.954 | 82.403 |  nan  |  nan  |\n",
      "\u001b[32m[11/10 17:30:52 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[11/10 17:30:52 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *segm*\n",
      "\u001b[32m[11/10 17:30:52 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "\u001b[32m[11/10 17:30:52 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[11/10 17:30:52 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.837\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.960\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.919\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.837\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.065\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.533\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.859\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.859\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "\u001b[32m[11/10 17:30:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:-----:|:-----:|\n",
      "| 83.706 | 96.026 | 91.942 | 83.707 |  nan  |  nan  |\n",
      "\u001b[32m[11/10 17:30:52 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[11/10 17:30:52 detectron2]: \u001b[0mEvaluation results for cell_acdc_validation_slim in csv format:\n",
      "\u001b[32m[11/10 17:30:52 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[11/10 17:30:52 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[11/10 17:30:52 d2.evaluation.testing]: \u001b[0mcopypaste: 82.3637,96.0255,91.9541,82.4033,nan,nan\n",
      "\u001b[32m[11/10 17:30:52 d2.evaluation.testing]: \u001b[0mcopypaste: Task: segm\n",
      "\u001b[32m[11/10 17:30:52 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[11/10 17:30:52 d2.evaluation.testing]: \u001b[0mcopypaste: 83.7055,96.0262,91.9417,83.7066,nan,nan\n",
      "\u001b[32m[11/10 17:30:52 detectron2]: \u001b[0mOrderedDict([('bbox', {'AP': 82.36370946314581, 'AP50': 96.02552315032712, 'AP75': 91.95406370296114, 'APs': 82.40333346241137, 'APm': nan, 'APl': nan}), ('segm', {'AP': 83.7055322065906, 'AP50': 96.02622435359191, 'AP75': 91.94167139278092, 'APs': 83.706624288589, 'APm': nan, 'APl': nan})])\n",
      "\u001b[32m[11/10 17:30:52 d2.utils.events]: \u001b[0m eta: 0:42:27  iter: 199  total_loss: 0.3987  loss_cls: 0.06863  loss_box_reg: 0.1727  loss_mask: 0.1254  loss_rpn_cls: 0.002149  loss_rpn_loc: 0.01724  lr: 3e-05  max_mem: 2759M\n",
      "\u001b[32m[11/10 17:32:12 d2.utils.events]: \u001b[0m eta: 0:18:50  iter: 219  total_loss: 0.3863  loss_cls: 0.07758  loss_box_reg: 0.163  loss_mask: 0.1229  loss_rpn_cls: 0.00155  loss_rpn_loc: 0.01936  lr: 3e-05  max_mem: 2759M\n",
      "\u001b[32m[11/10 17:33:36 d2.utils.events]: \u001b[0m eta: 0:18:01  iter: 239  total_loss: 0.4196  loss_cls: 0.07975  loss_box_reg: 0.1781  loss_mask: 0.1272  loss_rpn_cls: 0.001989  loss_rpn_loc: 0.01858  lr: 3e-05  max_mem: 2759M\n",
      "\u001b[32m[11/10 17:35:10 d2.utils.events]: \u001b[0m eta: 0:18:51  iter: 259  total_loss: 0.3778  loss_cls: 0.06822  loss_box_reg: 0.1633  loss_mask: 0.1226  loss_rpn_cls: 0.001271  loss_rpn_loc: 0.02123  lr: 3e-05  max_mem: 2759M\n",
      "\u001b[32m[11/10 17:36:51 d2.utils.events]: \u001b[0m eta: 0:18:30  iter: 279  total_loss: 0.3808  loss_cls: 0.07793  loss_box_reg: 0.1687  loss_mask: 0.1183  loss_rpn_cls: 0.001918  loss_rpn_loc: 0.01943  lr: 3e-05  max_mem: 2759M\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/10 17:38:33 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[11/10 17:38:33 d2.data.datasets.coco]: \u001b[0mLoaded 241 images in COCO format from ./data/dataInCOCO/test/cell_acdc_coco_ds.json\n",
      "\u001b[32m[11/10 17:38:33 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[11/10 17:38:33 d2.data.common]: \u001b[0mSerializing 100 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[11/10 17:38:33 d2.data.common]: \u001b[0mSerialized dataset takes 2.50 MiB\n",
      "\u001b[32m[11/10 17:38:33 d2.evaluation.evaluator]: \u001b[0mStart inference on 100 batches\n",
      "\u001b[32m[11/10 17:38:48 d2.evaluation.evaluator]: \u001b[0mInference done 11/100. Dataloading: 0.0015 s/iter. Inference: 1.1564 s/iter. Eval: 0.0017 s/iter. Total: 1.1597 s/iter. ETA=0:01:43\n",
      "\u001b[32m[11/10 17:38:54 d2.evaluation.evaluator]: \u001b[0mInference done 16/100. Dataloading: 0.0019 s/iter. Inference: 1.1909 s/iter. Eval: 0.0019 s/iter. Total: 1.1952 s/iter. ETA=0:01:40\n",
      "\u001b[32m[11/10 17:38:59 d2.evaluation.evaluator]: \u001b[0mInference done 20/100. Dataloading: 0.0021 s/iter. Inference: 1.2166 s/iter. Eval: 0.0017 s/iter. Total: 1.2209 s/iter. ETA=0:01:37\n",
      "\u001b[32m[11/10 17:39:05 d2.evaluation.evaluator]: \u001b[0mInference done 25/100. Dataloading: 0.0020 s/iter. Inference: 1.1841 s/iter. Eval: 0.0017 s/iter. Total: 1.1883 s/iter. ETA=0:01:29\n",
      "\u001b[32m[11/10 17:39:10 d2.evaluation.evaluator]: \u001b[0mInference done 30/100. Dataloading: 0.0020 s/iter. Inference: 1.1732 s/iter. Eval: 0.0017 s/iter. Total: 1.1775 s/iter. ETA=0:01:22\n",
      "\u001b[32m[11/10 17:39:16 d2.evaluation.evaluator]: \u001b[0mInference done 35/100. Dataloading: 0.0019 s/iter. Inference: 1.1657 s/iter. Eval: 0.0017 s/iter. Total: 1.1699 s/iter. ETA=0:01:16\n",
      "\u001b[32m[11/10 17:39:21 d2.evaluation.evaluator]: \u001b[0mInference done 40/100. Dataloading: 0.0021 s/iter. Inference: 1.1427 s/iter. Eval: 0.0018 s/iter. Total: 1.1471 s/iter. ETA=0:01:08\n",
      "\u001b[32m[11/10 17:39:26 d2.evaluation.evaluator]: \u001b[0mInference done 45/100. Dataloading: 0.0021 s/iter. Inference: 1.1321 s/iter. Eval: 0.0018 s/iter. Total: 1.1365 s/iter. ETA=0:01:02\n",
      "\u001b[32m[11/10 17:39:32 d2.evaluation.evaluator]: \u001b[0mInference done 50/100. Dataloading: 0.0021 s/iter. Inference: 1.1281 s/iter. Eval: 0.0017 s/iter. Total: 1.1324 s/iter. ETA=0:00:56\n",
      "\u001b[32m[11/10 17:39:37 d2.evaluation.evaluator]: \u001b[0mInference done 55/100. Dataloading: 0.0022 s/iter. Inference: 1.1265 s/iter. Eval: 0.0017 s/iter. Total: 1.1308 s/iter. ETA=0:00:50\n",
      "\u001b[32m[11/10 17:39:43 d2.evaluation.evaluator]: \u001b[0mInference done 60/100. Dataloading: 0.0021 s/iter. Inference: 1.1242 s/iter. Eval: 0.0017 s/iter. Total: 1.1285 s/iter. ETA=0:00:45\n",
      "\u001b[32m[11/10 17:39:48 d2.evaluation.evaluator]: \u001b[0mInference done 65/100. Dataloading: 0.0022 s/iter. Inference: 1.1142 s/iter. Eval: 0.0016 s/iter. Total: 1.1185 s/iter. ETA=0:00:39\n",
      "\u001b[32m[11/10 17:39:53 d2.evaluation.evaluator]: \u001b[0mInference done 70/100. Dataloading: 0.0022 s/iter. Inference: 1.1106 s/iter. Eval: 0.0016 s/iter. Total: 1.1150 s/iter. ETA=0:00:33\n",
      "\u001b[32m[11/10 17:39:59 d2.evaluation.evaluator]: \u001b[0mInference done 75/100. Dataloading: 0.0022 s/iter. Inference: 1.1062 s/iter. Eval: 0.0016 s/iter. Total: 1.1105 s/iter. ETA=0:00:27\n",
      "\u001b[32m[11/10 17:40:04 d2.evaluation.evaluator]: \u001b[0mInference done 80/100. Dataloading: 0.0023 s/iter. Inference: 1.1024 s/iter. Eval: 0.0016 s/iter. Total: 1.1067 s/iter. ETA=0:00:22\n",
      "\u001b[32m[11/10 17:40:09 d2.evaluation.evaluator]: \u001b[0mInference done 85/100. Dataloading: 0.0022 s/iter. Inference: 1.1037 s/iter. Eval: 0.0016 s/iter. Total: 1.1080 s/iter. ETA=0:00:16\n",
      "\u001b[32m[11/10 17:40:15 d2.evaluation.evaluator]: \u001b[0mInference done 90/100. Dataloading: 0.0022 s/iter. Inference: 1.0997 s/iter. Eval: 0.0016 s/iter. Total: 1.1040 s/iter. ETA=0:00:11\n",
      "\u001b[32m[11/10 17:40:20 d2.evaluation.evaluator]: \u001b[0mInference done 95/100. Dataloading: 0.0023 s/iter. Inference: 1.0957 s/iter. Eval: 0.0016 s/iter. Total: 1.1001 s/iter. ETA=0:00:05\n",
      "\u001b[32m[11/10 17:40:25 d2.evaluation.evaluator]: \u001b[0mInference done 100/100. Dataloading: 0.0022 s/iter. Inference: 1.0934 s/iter. Eval: 0.0016 s/iter. Total: 1.0977 s/iter. ETA=0:00:00\n",
      "\u001b[32m[11/10 17:40:25 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:01:44.376109 (1.098696 s / iter per device, on 1 devices)\n",
      "\u001b[32m[11/10 17:40:25 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:43 (1.093379 s / iter per device, on 1 devices)\n",
      "\u001b[32m[11/10 17:40:25 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[11/10 17:40:25 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/al_pipeline_config2/coco_instances_results.json\n",
      "\u001b[32m[11/10 17:40:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[11/10 17:40:25 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[11/10 17:40:25 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "\u001b[32m[11/10 17:40:25 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[11/10 17:40:25 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.826\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.960\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.920\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.826\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.064\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.525\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.851\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.851\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "\u001b[32m[11/10 17:40:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:-----:|:-----:|\n",
      "| 82.572 | 96.023 | 91.953 | 82.618 |  nan  |  nan  |\n",
      "\u001b[32m[11/10 17:40:25 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[11/10 17:40:25 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *segm*\n",
      "\u001b[32m[11/10 17:40:25 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "\u001b[32m[11/10 17:40:25 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[11/10 17:40:25 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.827\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.960\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.908\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.827\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.064\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.527\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.849\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.849\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "\u001b[32m[11/10 17:40:25 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm  |  APl  |\n",
      "|:------:|:------:|:------:|:------:|:-----:|:-----:|\n",
      "| 82.732 | 96.025 | 90.845 | 82.734 |  nan  |  nan  |\n",
      "\u001b[32m[11/10 17:40:25 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[11/10 17:40:25 detectron2]: \u001b[0mEvaluation results for cell_acdc_validation_slim in csv format:\n",
      "\u001b[32m[11/10 17:40:25 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[11/10 17:40:25 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[11/10 17:40:25 d2.evaluation.testing]: \u001b[0mcopypaste: 82.5716,96.0227,91.9532,82.6180,nan,nan\n",
      "\u001b[32m[11/10 17:40:25 d2.evaluation.testing]: \u001b[0mcopypaste: Task: segm\n",
      "\u001b[32m[11/10 17:40:25 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[11/10 17:40:25 d2.evaluation.testing]: \u001b[0mcopypaste: 82.7322,96.0248,90.8451,82.7336,nan,nan\n",
      "\u001b[32m[11/10 17:40:25 detectron2]: \u001b[0mOrderedDict([('bbox', {'AP': 82.57159467963578, 'AP50': 96.02272871844487, 'AP75': 91.95323555001873, 'APs': 82.61795737362995, 'APm': nan, 'APl': nan}), ('segm', {'AP': 82.73222657041856, 'AP50': 96.024796673836, 'AP75': 90.84510440361588, 'APs': 82.73364823980901, 'APm': nan, 'APl': nan})])\n",
      "\u001b[32m[11/10 17:40:25 d2.utils.events]: \u001b[0m eta: 0:35:43  iter: 299  total_loss: 0.4044  loss_cls: 0.07219  loss_box_reg: 0.1662  loss_mask: 0.1205  loss_rpn_cls: 0.00229  loss_rpn_loc: 0.01725  lr: 3e-05  max_mem: 2759M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cfg = get_config(\"al_pipeline_config2\")\n",
    "al_trainer = ActiveLearningTrainer(cfg)\n",
    "al_trainer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d364eb-d55a-46a8-9d41-0e205373bb1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
