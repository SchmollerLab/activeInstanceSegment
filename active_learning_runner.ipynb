{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f796e45c-0cb0-4df9-8367-96b60ab4c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "import random as rd\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "import wandb\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.config.config import CfgNode as CN\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "\n",
    "from src.globals import *\n",
    "from src.visualization.show_image import show_image\n",
    "from src.register_datasets import register_datasets, register_by_ids\n",
    "from src.test import do_test\n",
    "from src.train import do_train\n",
    "from src.predict import predict_image_in_acdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd9bd4a-ad68-4883-81e4-e6bf9bf4a485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_config():\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "    cfg.NAME = \"al_pipeline_config\"\n",
    "    cfg.AL = CN()\n",
    "    cfg.AL.DATASETS = CN()\n",
    "    cfg.AL.DATASETS.TRAIN_UNLABELED = TRAIN_DATASET_FULL\n",
    "    cfg.AL.MAX_LOOPS = 20\n",
    "    cfg.AL.INIT_SIZE = 20\n",
    "    cfg.AL.INCREMENT_SIZE = 20\n",
    "    cfg.AL.QUERY_STRATEGY = RANDOM\n",
    "    \n",
    "    cfg.DATASETS.TRAIN = (TRAIN_DATASET_FULL,)    \n",
    "    cfg.DATASETS.TEST = (VALIDATION_DATASET_SLIM,)\n",
    "    cfg.DATALOADER.NUM_WORKERS = 2\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "    cfg.SOLVER.IMS_PER_BATCH = 2  # This is the real \"batch size\" commonly known to deep learning people\n",
    "    cfg.SOLVER.BASE_LR = 0.0003  # pick a good LR\n",
    "    cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
    "    cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
    "    cfg.WARMUP_ITERS = 0\n",
    "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # The \"RoIHead batch size\". 128 is faster, and good enough for this toy dataset (default: 512)\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "    cfg.OUTPUT_DIR = \"./output/src/pipeline_configs/\" + cfg.NAME\n",
    "    cfg.TEST.EVAL_PERIOD = 0\n",
    "    \n",
    "    with open(\"./src/pipeline_configs/\" + cfg.NAME + \".yaml\",\"w\") as file:\n",
    "        file.write(cfg.dump())\n",
    "\n",
    "def get_config(config_name):\n",
    "    \n",
    "    cfg = get_cfg()\n",
    "    cfg.NAME = \" \"\n",
    "    cfg.AL = CN()\n",
    "    cfg.AL.DATASETS = CN()\n",
    "    cfg.AL.DATASETS.TRAIN_UNLABELED = \"\"\n",
    "    cfg.AL.MAX_LOOPS = 0\n",
    "    cfg.AL.INIT_SIZE = 0\n",
    "    cfg.AL.INCREMENT_SIZE = 0\n",
    "    cfg.AL.QUERY_STRATEGY = \"\"\n",
    "    cfg.WARMUP_ITERS = 0\n",
    "    \n",
    "    file_path = \"src/pipeline_configs/\" + config_name + \".yaml\"\n",
    "    cfg.merge_from_file(file_path)\n",
    "    return cfg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f560119c-4719-4509-ae35-bd7b884f31db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "from src.register_datasets import register_datasets, register_by_ids\n",
    "\n",
    "class ActiveLearingDataset:\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.cfg = cfg\n",
    "        \n",
    "        register_datasets()\n",
    "\n",
    "        # get ids of all images\n",
    "        self.unlabeled_ids = [image[\"image_id\"] for image in DatasetCatalog.get(cfg.AL.DATASETS.TRAIN_UNLABELED)]\n",
    "        self.labeled_ids = []\n",
    "        \n",
    "        self.unlabeled_data_name = \"temp_unlabeled_data_al\"\n",
    "        self.labeled_data_name = \"temp_labeled_data_al\"\n",
    "        \n",
    "        self.init_size = cfg.AL.INIT_SIZE\n",
    "        self.increment_size = cfg.AL.INCREMENT_SIZE\n",
    "        \n",
    "        # set seed\n",
    "        rd.seed(1337)\n",
    "        sample_ids = rd.sample(self.unlabeled_ids, self.init_size)\n",
    "        self.update_labeled_data(sample_ids)\n",
    "        self.get_labeled_dataset()\n",
    "        self.get_unlabled_dataset()\n",
    "        \n",
    "    \n",
    "    def remove_data_from_catalog(self,name):\n",
    "        \n",
    "        if name in DatasetCatalog:\n",
    "            DatasetCatalog.remove(name)\n",
    "            MetadataCatalog.remove(name)\n",
    "        \n",
    "        \n",
    "    def get_labeled_dataset(self):\n",
    "        self.remove_data_from_catalog(self.labeled_data_name)\n",
    "        register_by_ids(self.cfg, self.labeled_data_name, self.labeled_ids)\n",
    "        self.cfg.DATASETS.TRAIN = (self.labeled_data_name,)\n",
    "    \n",
    "    def get_unlabled_dataset(self):\n",
    "        self.remove_data_from_catalog(self.unlabeled_data_name)\n",
    "        register_by_ids(self.cfg, self.unlabeled_data_name,self.unlabeled_ids)\n",
    "        self.cfg.AL.DATASETS.TRAIN_UNLABELED = self.unlabeled_data_name\n",
    "    \n",
    "    def update_labeled_data(self, sample_ids):\n",
    "        print(\"update_labeled_data\")\n",
    "        # check if sample_ids are in unlabeled_ids\n",
    "        if not (set(sample_ids) <= set(self.unlabeled_ids)):\n",
    "            raise Exception(\"Some ids ({}) in sample_ids are not contained in unlabeled data pool: {}\".format(len(list(set(sample_ids) - set(self.unlabeled_ids))),list(set(sample_ids) - set(self.unlabeled_ids))[:5])) \n",
    "\n",
    "        self.labeled_ids += sample_ids\n",
    "        self.unlabeled_ids = list(set(self.unlabeled_ids) - set(sample_ids))\n",
    "        \n",
    "        self.get_labeled_dataset()\n",
    "        self.get_unlabled_dataset()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d0e31c-3dd9-46b4-b3ef-0e81e1b3f6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "class QueryStrategy(object):\n",
    "    \n",
    "    def __init__(self,cfg):\n",
    "        \n",
    "        self.cfg = cfg\n",
    "        \n",
    "    \n",
    "    def sample(self,model, ids):\n",
    "        pass\n",
    "    \n",
    "class RandomSampler(QueryStrategy):\n",
    "    \n",
    "    def sample(self,model, ids):\n",
    "        num_samples = self.cfg.AL.INCREMENT_SIZE        \n",
    "        samples = rd.sample(ids, num_samples)\n",
    "        return samples\n",
    "\n",
    "class GTknownSampler(QueryStrategy):\n",
    "    \n",
    "    def sample(self, model, ids):\n",
    "        num_samples = self.cfg.AL.INCREMENT_SIZE        \n",
    "        id_pool = rd.sample(ids, 300)\n",
    "        \n",
    "        register_by_ids(self.cfg,\"GTknownSampler_DS\",id_pool)\n",
    "\n",
    "        \n",
    "        evaluator = COCOEvaluator(\"GTknownSampler_DS\", output_dir=self.cfg.OUTPUT_DIR)\n",
    "        data_loader = build_detection_test_loader(self.cfg, \"GTknownSampler_DS\")\n",
    "        inference_on_dataset(model, data_loader, evaluator)\n",
    "\n",
    "\n",
    "        result_array = []\n",
    "        image_ids = [image[\"image_id\"] for image in DatasetCatalog.get(\"GTknownSampler_DS\")]\n",
    "        for image_id in image_ids:\n",
    "            result = evaluator.evaluate(image_id)\n",
    "            result_array.append(result)\n",
    "\n",
    "        aps = np.array([result['segm']['AP'] for result in result_array])\n",
    "        sample_ids = list(np.argsort(aps)[:num_samples])\n",
    "        \n",
    "        samples = [image_ids[id] for id in sample_ids]\n",
    "\n",
    "        return samples\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17e6749-87d6-4145-874e-dd06daffa29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ActiveLearningTrainer:\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        # initialize weights and biases\n",
    "        wandb.init(project=\"activeCell-ACDC\", sync_tensorboard=True)\n",
    "        \n",
    "        self.logger = setup_logger(output=\"./log/main.log\")\n",
    "        self.logger.setLevel(10)\n",
    "        \n",
    "        self.al_dataset = ActiveLearingDataset(cfg)   \n",
    "        self.model = build_model(cfg)\n",
    "        self.query_strategy = RandomSampler(cfg)\n",
    "        \n",
    "        \n",
    "    def __del__(self):\n",
    "        wandb.run.finish()\n",
    "    \n",
    "    def step(self, resume):\n",
    "        \n",
    "        len_ds_train = len(DatasetCatalog.get(self.cfg.DATASETS.TRAIN[0]))\n",
    "        print(\"lenght of train data set: {}\".format(len_ds_train))\n",
    "        self.cfg.SOLVER.MAX_ITER = len_ds_train*20\n",
    "        self.cfg.SOLVER.STEPS = [len_ds_train*10]\n",
    "        \n",
    "        do_train(self.cfg, self.model, self.logger,resume=resume)\n",
    "        result = do_test(self.cfg, self.model, self.logger)\n",
    "\n",
    "        sample_ids = self.query_strategy.sample(self.model, self.al_dataset.unlabeled_ids)\n",
    "        self.al_dataset.update_labeled_data(sample_ids)\n",
    "        \n",
    "    \n",
    "    def run(self):\n",
    "        try:\n",
    "            for i in range(self.cfg.AL.MAX_LOOPS):\n",
    "                self.step(resume=(i>0))\n",
    "        except Exception as e:\n",
    "            wandb.run.finish()\n",
    "            raise e\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba47561-2b0b-4853-b1e9-81d20f3f7f47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "build_config()\n",
    "cfg = get_config(\"al_pipeline_config\")\n",
    "al_trainer = ActiveLearningTrainer(cfg)\n",
    "al_trainer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747c2fab-4b95-41a7-81cd-6ddc14d5eaa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
